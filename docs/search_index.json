[
["index.html", "An Analysis of Groundwater Levels in the Central Valley of California Chapter 1 Abstract", " An Analysis of Groundwater Levels in the Central Valley of California Michael Stevens 2020-12-17 Chapter 1 Abstract As groundwater is an essential and over-utilized resource in dry climates like the Western United States, responsibly managing this resource becomes crucial for sustainability. In-situ groundwater level data are often inconsistent, which makes temporal analysis difficult. There are many interpolation methods that can be used to fill in gaps in datasets. Evans, Williams, Jones, Ames, and Nelson (2020) demonstrated an effective imputation method for groundwater data, utilizing an Extreme Learning Machine (ELM). This study will explore the effectiveness of this approach in the Central Valley of California, an extremely productive agricultural area facing groundwater depletion challenges. A study compiled by NASA Jet Propulsion Lab (JPL) compiled a robust dataset of in-situ groundwater level data in the Central Valley. This dataset will be analyzed for appropriate test areas and wells to compare the efficacy of the ELM method in this region against the IDW spatial interpolation method. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction A study performed by the NASA Jet Propulsion Laboratory (JPL) compiled data entries from over a million wells in the central valley. From this list, representative wells (with a sufficient amount of measurements) were chosen for 1-km grids across the valley. This dataset includes measurements for the depth to groundwater at each well, measured across varying ranges and intervals of time. The data consists of 9 different variable. Five of these variables relate to the identification number of each well. Two variables relate to the spatial location (lat/long coordinates) of each well. The wells are distributed all across the Central Valley of California. The final two variables describe the date that each measurement was taken and the value of the measurement itself. The well data begins around 2001 and ends in 2019. Depth to groundwater varies from well to well, but is generally between 0 and 100 feet. It will be necessary to retrieve ground surface elevation (GSE) data for each well so that the Water Table Elevation (WTE) can be determined, which is the difference between the GSE and the depth to groundwater. In this study, these measurements will be trimmed down to a smaller area within the valley that will serve as a test case for our study. The area will be chosen based on the spatial and temporal robustness of its data. "],
["literature-review.html", "Chapter 3 Literature Review", " Chapter 3 Literature Review Correctly characterizing groundwater levels and storage in an aquifer can be very difficult without a robust dataset. Often, in-situ data are collected at inconsistent intervals, while wells and monitoring stations are rarely distributed evenly. Because groundwater depends on so many different factors and does not exhibit linear behavior, finding ways to estimate groundwater levels without a complete set of in-situ data becomes a very important task. Some of these methods will be discussed here. Many of these methods utilize remote sensing data to analyze groundwater levels. A study performed by Thomas et al. (2017) utilizes data from NASA’s Gravity Recovery and Climate Experiment (GRACE) mission to determine changes in groundwater. Satellites measure the changes in Earth’s gravitational field, due to changes in mass distribution, which can measure the redistribution of water. The study normalizes data collected by GRACE to measure changes in groundwater due to drought, which they call the GRACE Groundwater Drought Index (GGDI). Many measures can only account for climatological factors, but the GGDI is able to capture anthropogenic effects as well. This important for groundwater, which is often subjected to increased pumping during droughts. The method demonstrated an effective way to utilize remotely sensed data across a large area to measure groundwater response. However, the authors note that, due to GRACE’s temporal resolution, the GGDI can only be used with confidence to determine droughts after 3 months of indicated drought. There are other methods to utilize remotely sensed data to characterize groundwater. Sneed (2015) used remotely sensed data to track land subsidence during 2007-2014 of over 0.5 m in the San Joaquin Valley (which falls within the Central Valley of California). Land surface depressions were determined using Interferometric Synthetic Aperture Radar (InSAR) and in-situ data from extensometers and Continuous Global Positioning System (CGPS) data. This increase in subsidence coincided with an increase in groundwater pumping, due to drought. These findings agree with a study performed by Vasco (2019), which analyzed a drop in vertical land surface in the Central Valley during 2015-2018, also observed using InSAR data. Their estimated decrease in groundwater storage in the Tulare basin was similar to other independent estimates, including that of the GRACE mission, another remote sensing data source. This provides reasonable assurances on the accuracy of the method for this area. These findings show promise for the efficacy of using remotely sensed data to measure changes in groundwater levels in the Central Valley. Another method for estimating groundwater levels is interpolating measurements based on in-situ data. Since many wells lack consistent and continuous measurements, statistical machine learning methods can be used to fill in these gaps. One such method is the Extreme Learning Machine (ELM), which was described by Huang, Zhu, and Siew (2006). The ELM is a type of machine learning that can map nonlinear relationships between input data sources. These correlations can then be used to determine missing values in one of the data sources. The ELM method was demonstrated to have a very fast learning time and is flexible to use in many different applications. This particular study has been cited over 7000 times. If the method can be demonstrably more effective and accurate than other methods of data imputation in a groundwater application, it could prove to be a very useful tool. This ELM method was demonstrated in a groundwater application in a recent study by Evans (2020). They demonstrated a method to impute temporal gaps in groundwater data using remotely sensed Earth observation data and the in-situ data as the input datasets. Earth observation data sources included two soil moisture models: the Global Land Data Assimilation System (GLDAS) model and the NOAA Climate Prediction Center (CPC) model. It also included the Palmer Drought Severity Index (PDSI). The ELM was applied well-by-well and observed correlations between the groundwater level measurements from each well and the Earth observation data. This correlation was then used to interpolate data points during temporal ranges that had no data. The method was chosen due to its ability to function without a large amount of in-situ data to “train” the model, which is common among groundwater datasets. To demonstrate its effectiveness, the method was applied to two areas in Utah. The results obtained by the Earth observation method utilizing ELM were significantly more accurate than Kriging (a form a spatial interpolation) in areas of relatively consistent human influences (changes in groundwater pumping, land use changes, etc.). This method provides a more accurate and flexible method to impute groundwater data, which could be very helpful in areas with limited or especially sporadic groundwater data. However, only two areas were analyzed in the paper, demonstrating the need for further testing. The Central Valley of California is one of the most productive agricultural areas in the United States. The area produces approximately $17 billion in agronomic value per year and a quarter of the nation’s food (USGS (n.d.)). However, due to the dry climate that it sits in, farms in the valley rely heavily on groundwater. In fact, it is the 2nd most-pumped aquifer system in the country, according to the United States Geological Survey (USGS). In fact, groundwater levels in the valley are reaching historically low levels (Thomas et al. (2017)). Over pumping of aquifers can lead to severe consequences to both people and property through subsidence and can lead to long-term and even permanent changes in hydrologic and aquifer patterns. Due to this extreme amount of groundwater extraction in the area and the resultant threats, regulation now requires more sustainable use of groundwater in the region. Due to the wealth of groundwater data in the area, it is a perfect candidate for testing of groundwater interpolation methods. A recent study performed by the Jet Propulsion Laboratory at the California Institute of Technology recently performed a study that compiled all freely available groundwater data in the area from different sources and synthesized it. Data were collected from 5 different sources across two different agencies: the USGS and the California Department of Water Resources (DWR). They eliminated duplicates and invalid data, then used a scoring method to select wells that were evenly distributed and had an appropriately large and consistent set of observations. This study has not yet been published, and so will not be cited in this work. However, this dataset was shared with the author’s research group at BYU and will be used as the base for this study. References "],
["data-description.html", "Chapter 4 Data Description", " Chapter 4 Data Description This section will introduce and wrangle the data used in this project. We will first explore the data and the distribution of the data. Then, we will extract some important subsets of the data to use in the next chapter. This will involve identifying wells that would be good “test” candidates to test our methods of imputation. As described in the last section, the data used in this project comes from a recent study performed by NASA’s JPL. The study compiled all freely available groundwater data in the Central Valley of CA from different sources and synthesized it. Data were collected from 5 different sources across two different agencies: the USGS and the California Department of Water Resources (DWR). As noted, duplicates and wells without sufficient data were filtered out and a “representative” well was chosen for each square kilometer that had at least one well available. First, we can load the dataset and inspect. It contains 9 different variables, including 5 that relate to various identification numbers. There are lat/long variables describing the geographic reference, a depth to groundwater value, and a date at which the depth measurement was taken, as seen in the table below. load(&quot;Data/master.Rda&quot;) master &lt;- subset(master, select = c(&quot;NDI&quot;, &quot;mergeOn&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;date&quot;, &quot;depth.to.GW..ft.&quot;)) head(master) ## NDI mergeOn lat lon date depth.to.GW..ft. ## 1 0 3.23233E+14 32.54 -117.03 2010-03-04 22.93 ## 2 0 3.23233E+14 32.54 -117.03 2010-03-19 22.87 ## 3 0 3.23233E+14 32.54 -117.03 2010-05-06 21.84 ## 4 0 3.23233E+14 32.54 -117.03 2010-05-25 21.22 ## 5 0 3.23233E+14 32.54 -117.03 2010-07-09 20.76 ## 6 0 3.23233E+14 32.54 -117.03 2010-09-22 20.09 A histogram can show us the time distribution of measurements for the entire dataset. # Create a histogram of the entire dataset hist(master$date, &#39;years&#39;, xlab = &quot;Date&quot;, ylab = &quot;Number of Observations&quot;, freq = TRUE, format = &quot;%Y&quot;) To understand the spatial locations of these wells, we will need to display each well. However, in order to do this, we need to first extract the wells from the master dataset so that we are not mapping them multiple times. Once we have that, we can create a map using Leaflet that shows all of the wells in the dataset. A polygon outlining the central valley has also been included in blue. Wells have been “clustered” for a quicker processing time. library(dplyr) library(leaflet) # Create a master list of all wells master_wells &lt;- distinct(master, mergeOn, .keep_all = TRUE)[,1:4] # we must keep some columns as chr b/c some have letters master_wells$NDI &lt;- as.character(master_wells$NDI) # We&#39;ll save this dataframe for later write.csv(master_wells, file = &quot;Data/master_wells.csv&quot;) # Load in a geojson file that outlines the central valley CV_shape &lt;- geojsonio::geojson_read(&quot;code/CA_Bulletin_118_Aquifer_Regions_dissolved.geojson&quot;, what = &quot;sp&quot;) # Create a leaflet map of the all wells in the dataset with the central valley outline leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = CV_shape, stroke = FALSE, smoothFactor = 0.3, fillOpacity = 0.5) %&gt;% addMarkers(data = master_wells, lng = ~lon, lat = ~lat, popup = ~NDI, clusterOptions = markerClusterOptions()) In order to clean this data to make it more manageable, we are going to filter out all wells that do not have more than 180 observations and do not fall within the central valley boundary polygon. library(sf) library(tidyverse) # Find datasets that have at least 180 observations (enough data points for monthly data across 15 years - this is just a screening value) master_count &lt;- as.data.frame(table(master$mergeOn)) w180 &lt;- master_count %&gt;% filter(Freq &gt; 180) %&gt;% merge(master_wells, by.x = &quot;Var1&quot;, by.y = &quot;mergeOn&quot;) %&gt;% rename(mergeOn = Var1) # Find 180 wells that fall within CV boundaries # We&#39;ll transform the w180 and CV_shape data objects to simple feature geometry objects pts &lt;- st_as_sf(x = w180, coords = c(&quot;lon&quot;, &quot;lat&quot;)) st_crs(pts) &lt;- st_crs(CV_shape) CV_shape &lt;- st_as_sf(x = CV_shape) # Now we&#39;ll intersect the two data objects and create a data frame CV_wells &lt;- CV_shape %&gt;% st_intersection(pts) %&gt;% extract(col = geometry, into = c(&#39;long&#39;, &#39;lat&#39;), &#39;\\\\((.*), (.*)\\\\)&#39;, convert = TRUE) %&gt;% as.data.frame() %&gt;% subset(select = -c(Basin_Name, Shape_Length, Shape_Area, geometry)) %&gt;% subset(select = -c(geometry)) After an initial cleaning, the dataset can be further refined to find data that meet our criteria. For this analysis, we’d like as consistent of a database as possible, so we’ll filter out data that doesn’t span at least 15 years. library(lubridate) # Next, we need to find the wells that have data that span across at least 15 years. ## In addition, we would like to find data with an adequate distribution: ### We&#39;ll do this by looping through the data to find wells with at least 4 data points per year. w15 &lt;- list() for (well in CV_wells$mergeOn) { ts &lt;- filter(master, mergeOn == well) diff &lt;- as.integer(max(ts$date) - min(ts$date)) yr_dist &lt;- as.data.frame(table(year(ts$date))) add &lt;- TRUE if (diff &gt; 5475){ for (yr_freq in yr_dist$Freq) { if (yr_freq &lt; 4){ add = FALSE } } if (add == TRUE) { w15 &lt;- append(w15, well) } } } Finally, the data can be summarized into 3 month blocks. Then, wells with coverage over the entire timeframe (19 years in this case) will be kept for further analysis. Once those wells are identified, a proper spatial distribution will need to be determined. # Next, we&#39;ll find wells that have at least one measurement every 3 months for 19 years w76 &lt;- data.frame(c()) for (well in w15){ ts &lt;- filter(master, mergeOn == well) mon_ts &lt;- ts %&gt;% mutate(dategroup = lubridate::floor_date(date, &quot;3 months&quot;)) %&gt;% group_by(dategroup) %&gt;% summarize(Mean_depth=mean(depth.to.GW..ft.)) if (nrow(mon_ts) &gt;= 76){ w76 &lt;- rbind(w76, well) } } # This creates a dataframe with wells with data spanning 15 years w76 &lt;- w76 %&gt;% rename(mergeOn = colnames(w76)) %&gt;% merge(CV_wells, by = &quot;mergeOn&quot;) # Create another leaflet map of the wells that span 15+ years leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = CV_shape, stroke = FALSE, smoothFactor = 0.3, fillOpacity = 0.5) %&gt;% addMarkers(data = w76, lng = ~long, lat = ~lat, popup = ~mergeOn ) For this analysis, we need a small cluster of wells that are within a reasonable (~50 km) from each other. A certain distance could be set as a threshold, and R could be used to determine a proper cluster programmatically. However, in this case, it is easier to manually determine a good cluster of wells. Based on the map, wells 25N03W11B003M, 29N03W18M001M, 24N02W29N004M, 24N02W24D003M, and 24N02W03B001M were chosen (the area will be displayed a little later on). A facet-wrapped histogram shows the distribtion of these wells’ measurements. # From the map, we identified 5 wells that are grouped together. test_wells &lt;- c(&#39;25N03W11B003M&#39;, &#39;29N03W18M001M&#39;, &#39;24N02W29N004M&#39;, &#39;24N02W24D003M&#39;, &#39;24N02W03B001M&#39;) # Now we can create a time series list and generate a histogram mosaic test_ts &lt;- subset(master, mergeOn %in% test_wells) ggplot(test_ts, aes(x=date)) + geom_histogram(bins = 19*4) + facet_wrap(~mergeOn) These wells will make up the basis for the remainder of this study. This final map includes a red box identifying the 5 wells that were chosen. # Final leaflet map with study area in red leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = CV_shape, stroke = FALSE, smoothFactor = 0.3, fillOpacity = 0.5) %&gt;% addMarkers(data = w76, lng = ~long, lat = ~lat, popup = ~mergeOn ) %&gt;% addRectangles(lng1 = -121.4, lat1 = 38.75, lng2 = -122, lat2 = 39.1, color = &quot;#ff0000&quot;, opacity = 0.9, fillColor = &quot;transparent&quot;) It will be important to be able to retrieve this data later on. We will save the “test_wells” dataframe so that we can retrieve this data again in the next section. Additionally, we will need the ground surface elevations for each well in order to calculate water table elevations in the next section. These elevations were estimated manually from Google Earth. # Change test_wells to be a dataframe with well info test_wells &lt;- subset(master_wells, mergeOn %in% test_wells) # Ground surface elevations obtained using Google Earth test_wells$Elev &lt;- c(55,22,46,19,25) # Subset the data to necessary columns and save as csv test_wells &lt;- subset(test_wells, select = c(mergeOn,lat,lon,Elev)) write.csv(test_wells, file = &quot;Data/test_wells.csv&quot;) This concludes the data exploration chapter. The next chapter will explore different methods of groundwater level imputation. "],
["analysis.html", "Chapter 5 Analysis 5.1 Extreme Learning Machine 5.2 Inverse Distance Weighting 5.3 Error Comparison", " Chapter 5 Analysis In this chapter, we will clean the data further and perform interpolation on each well from the chosen set. First, the Extreme Learning Machine method, described in Chapter 3, will be set up to impute missing values. A spatial interpolation method, Inverse Distance Weighting (IDW) will then be performed for comparison purposes. Finally, the accuracy of the IDW method will be analyzed and compared. In order to determine accuracy, three years (2012-2015) of data will be imputed. Then, the imputed data will be compared to the observed data. 5.1 Extreme Learning Machine As described previously, the ELM method forms a relationship between input data sources (in our case, remotely sensed earth observation data) and observed data. This relationship is then used to impute data for time periods without measurements. Evans (2020) demonstrated this method in Python with promising results. Unfortunately, due to time constraints and knowledge constraints, the conversion from Python to R was not completed in this paper. The full scope of this undertaking was not evident to the author at the outset (it should be noted that Evans (2020) shared psuedocode in their paper; the full process is much more involved). However, in future work, we hope to be able to solve this method in R. This section will merely set up the data and methods that future work can build upon. First, we will need to load previously created datasets. # Load existing datasets load(&quot;Data/master.Rda&quot;) master_wells &lt;- read.csv(file = &#39;Data/master_wells.csv&#39;) test_wells &lt;- read.csv(file = &#39;Data/test_wells.csv&#39;) In order to prepare the data for analysis, the data must be summarized to uniformly spaced time steps. We will summarize this dataset to 3 month (quarterly) averages. In order to interpolate, water table elevations must be generated, rather than simple depth to groundwater measurements. This is because different depths may extend to the same water table elevation, depending on the ground elevation. This is less important for the ELM method, but becomes very important in spatial interpolation, discussed in the next section. # Create dataset with all well data in 3 month increments test_ts_3mon &lt;- data.frame(c()) for (well in test_wells$mergeOn){ ts &lt;- filter(master, mergeOn == well) w3mon_ts &lt;- ts %&gt;% mutate(date = lubridate::floor_date(date, &quot;3 months&quot;), mergeOn = mergeOn) %&gt;% group_by(date, mergeOn) %&gt;% summarize(Mean_depth=-1*mean(depth.to.GW..ft.)) test_ts_3mon &lt;- as.data.frame(rbind(test_ts_3mon, w3mon_ts)) #assign(paste(well, &quot;_3mon_ts&quot;, sep = &quot;&quot;), mon_ts) } # Format time series dataframe and add WTE column test_ts_3mon &lt;- merge(test_ts_3mon, test_wells, by = &quot;mergeOn&quot;) test_ts_3mon$WTE &lt;- test_ts_3mon$Elev + test_ts_3mon$Mean_depth test_ts_3mon &lt;- subset(test_ts_3mon, select = -c(X)) write.csv(test_ts_3mon, file = &quot;Data/test_ts_3mon.csv&quot;) Now, we will create a facet-wrapped group of time series graphs for each well using the quarterly averages that we just created. # Now let&#39;s create time series graphs of these quarterly means test_ts_3mon %&gt;% ggplot(aes(x = date, y = WTE)) + geom_line(color = &quot;blue&quot;) + xlab(&quot;Date&quot;) + ylab(&quot;Water Table Elevation (ft above sea level)&quot;) + ggtitle(&quot;Quarterly Time Series for Each well&quot;) + theme(plot.title = element_text(hjust = 0.5)) + facet_wrap(~mergeOn) Next, we will load in a remotely sensed earth observation dataset, the Palmer Drought Severity Index (PDSI). The PDSI is one of the measures that we can correlate groundwater levels with. For the sake of this demonstration, we will only use the PDSI. But for more accurate results, multiple earth observation datasets would need to be used. The PDSI data is hosted by the National Oceanic and Atmospheric Administration’s (NOAA) Physical Sciences Laboratory (Dai and Qian (n.d.)). It spans from 1850 to 2014. There is a normal dataset and a “self-calibrated” dataset; we will use the latter because it has more recent data (it spans up through 2014, whereas the normal dataset spans up through 2005). It is stored in a netcdf file, which means we will need to utilize the ncdf4 library. We will start by opening the data file and store it in a data object. We will then print the metadata and inspect. # Store pdsi data in dataframe and print metadata library(ncdf4) climate_output_sc &lt;- nc_open(&quot;Data/pdsi.mon.mean.selfcalibrated.nc&quot;) print(climate_output_sc) ## File Data/pdsi.mon.mean.selfcalibrated.nc (NC_FORMAT_CLASSIC): ## ## 1 variables (excluding dimension variables): ## float pdsi[lon,lat,time] ## statistic: Mean ## missing_value: -99999 ## dataset: Dai Palmer Drought Severity Index: Self-calibrated ## long_name: Monthly Self-calibrated Palmer Drought Severity Index using Penman-Monteith PE ## level_desc: Surface ## var_desc: Palmer Drought Severity Index ## least_significant_digit: 2 ## units: Standardized Units of Relative Dry and Wet ## actual_range: -8.16020011901855 ## actual_range: 8.16014289855957 ## valid_range: -100 ## valid_range: 100 ## ## 3 dimensions: ## lon Size:144 ## units: degrees_east ## long_name: Longitude ## actual_range: -178.75 ## actual_range: 178.75 ## standard_name: longitude ## axis: X ## lat Size:55 ## units: degrees_north ## long_name: Latitude ## actual_range: -58.75 ## actual_range: 76.25 ## standard_name: latitude ## axis: Y ## time Size:1980 *** is unlimited *** ## units: hours since 1800-01-01 00:00:0.0 ## actual_range: 438288 ## actual_range: 1883904 ## long_name: Time ## delta_t: 0000-01-00 00:00:00 ## avg_period: 0000-01-00 00:00:00 ## standard_name: time ## axis: T ## ## 9 global attributes: ## title: Global Monthly Dai Palmer Drought Severity Index ## history: created Apr 2013 from data at NCAR webpage:updated Nov 2015 from new dataset version ## References: https://www.psl.noaa.gov/data/gridded/data.pdsi.html ## original_source: NCAR/UCAR: A Dai http://www.cgd.ucar.edu/cas/catalog/climind/pdsi.html. ## comments: original creation date: Thu Oct 25 15:25:40 MDT 2012 at NCAR. Updated at PSD Oct 2016 with corrected NCAR data ## Conventions: COARDS ## details: see ncar for more detials and updates ## description: ## Monthly Self-calibrated Palmer Drought Severity Index (scPDSI) ## calculated using observed surface air temperature (HadCRUT4 from ## http://www.cru.uea.ac.uk/cru/data/temperature/ ) and ## precipitation (from Dai et al. (1997, J.Clim: for 1870-1947) + ## Chen et al. (2002, J. Hydromet.: for 1948-1978 + GPCP v2.2 for ## 1979-present. The Dai and Chen P data were adjusted to have the ## same mean as the GPCP data over the 1979-1996 period). ## Calibration (or reference) period is 1950-1979. ## Wind speed and air pressure data were from 20CR v2. Other data ## were based on CRU TS3.22. Documention: ## Dai, A., 2011a: Characteristics and trends in various forms of the ## Palmer Drought Severity Index (PDSI) during 1900-2008. J. Geophys. ## Res., 116, D12115, doi:10.1029/2010JD015541. ## Dai, A., K. E. Trenberth, and T. Qian, 2004: A global data set of ## Palmer Drought Severity Index for 1870-2002: Relationship with soil ## moisture and effects of surface warming. J. Hydrometeorology, 5, ## 1117-1130. Data source: Dr. A. Dai/NCAR (adai@ucar.edu). See ## http://www.cgd.ucar.edu/cas/catalog/climind/pdsi.html for updates. ## WARNING: PDSI and scPDSI over the higher latitudes (e.g., &gt;50deg.) ## may not be a good proxy of soil moisutre content. Also, PDSI and ## scPDSI are highly autocorrelated indices not good at resolving ## sub-seasonal variations. Please use with caution! ## This is a normalized version so that every grid box has the same ## s.d. as that in the central U.S. during the calibr. period (1950-79) ## dataset_title: Palmer Drought Severity Index The important takeaway from this metadata is that there are three different variables - latitude, longitude, and time - over which the PDSI data spans across. This is why it was stored in a netCDF file format. We will store each of these variables in separate arrays. We will also store the PDSI values in an array and extract the values that we need based on the lat/long/time variables. We will start by obtaining an the latitude and longitude for our test well, which will be well 29N03W18M001M. This will be used to extract only the data we need from the pdsi dataset. Then the arrays can be created. # Create lat/long objects that will be used to obtain the pdsi data for the study area well_lat &lt;- mean(filter(test_wells, mergeOn == well)$lat) well_long &lt;- mean(filter(test_wells, mergeOn == well)$lon) # Obtain the lat,long, and time arrays from the data lat &lt;- ncvar_get(climate_output_sc,&quot;lat&quot;) lon &lt;- ncvar_get(climate_output_sc,&quot;lon&quot;) time &lt;- ncvar_get(climate_output_sc,&quot;time&quot;) # Obtain pdsi data and store in array pdsi_array &lt;- ncvar_get(climate_output_sc,&quot;pdsi&quot;) # Find the lat/long coordinates that are closest to the average well_long and well_lat ncd_lon &lt;- which(abs(lon-well_long)==min(abs(lon-well_long))) ncd_lat &lt;- which(abs(lat-well_lat)==min(abs(lat-well_lat))) #Print them cat(&quot;Lat:&quot;, lat[ncd_lat], &quot;, Long:&quot;, lon[ncd_lon]) ## Lat: 38.75 , Long: -121.25 Now that the data is set up, we will explore - briefly - the process for ELM. As this analysis has not been performed successfully in R, we will not spend an inordinate amount of time on it, except to introduce the concept. The detailed explanation of this method can be found in the study performed by Evans (2020) and all clarification should be sought in that paper. This report merely sets up the process. First, we generate a set of random input weights. These will have a normal distribution and will create a 1 x 500 matrix. Next, the bias vector is generated with 500 randomized values (creating a 500 x 1 matrix). Then, we will extract a time series for the study area, which is the lat/long coordinate in the PDSI array that is closest to the well. The well we will be using is Well #29N03W18M001M. # generate random input weights, W1 W1 &lt;- matrix(rnorm(1*500), 1, 500) # generate bias vector, b b &lt;- rnorm(500) # Create a time series of the PDSI data for the selected lat/long ncdf_ts &lt;- pdsi_array[ncd_lon,ncd_lat,] ncdf_ts &lt;- mutate(as.data.frame(ncdf_ts), date = time, PDSI = ncdf_ts, .keep = &quot;none&quot;) ncdf_ts$date &lt;- as.Date(ncdf_ts$date/24, origin = &quot;1800-01-01&quot;) tail(ncdf_ts) ## date PDSI ## 1975 2014-07-01 -1.96039844 ## 1976 2014-08-01 -0.36029744 ## 1977 2014-09-01 -0.06562341 ## 1978 2014-10-01 -0.65678751 ## 1979 2014-11-01 -0.76688999 ## 1980 2014-12-01 0.30445653 As we can see, the PDSI values show up and correlate to monthly time steps. To simplify the analysis and eliminate noise, we will re-sample the data to three-month intervals. We will also clip the data to the time period that we have observed groundwater level data for. # Create a ts dataframe of the PDSI data with 3 month averages and a time frame lining up with the well data X &lt;- c() X &lt;- ncdf_ts %&gt;% mutate(date = lubridate::floor_date(date, &quot;3 months&quot;)) %&gt;% group_by(date) %&gt;% summarize(PDSI=mean(PDSI)) %&gt;% subset(date %in% w3mon_ts$date) Now we can create the A Matrix. This matrix is used to determine the weights for the observed data correlation. # generate the A matrix a &lt;- (X$PDSI %*% W1) ab &lt;- a + rep(b, each = nrow(a)) # basis function (theta) sets negative values to zero A &lt;- pmax(ab, 0) The last thing we will do in this analysis is to generate the identity matrix. We will also apply a weight, lambda, to the identity matrix. I = diag(1) lamb = 100 lamb_I &lt;- lamb*I This concludes the set up for the ELM method. Next, we will discuss a spatial interpolation method, IDW, which will provide a comparison tool for the ELM. In future works, we will complete this analysis of ELM. 5.2 Inverse Distance Weighting Previously, we discussed spatial interpolation methods, such as Kriging. Inverse Distance Weighting (IDW) is essentially a simplified version of Kriging. IDW is a spatial interpolation method that uses nearby observations to interpolate an unknown value at a location. IDW places more weight on observed values that are closer to the point in question. It is a fairly simple method of interpolation, and can be implemented in R fairly easily, as will be shown. We will first interpolate the values over a predefined set of years and then compare the IDW solutions with the observed data. First, we will add the datasets created previously. As stated above, we will interpolate over 2012-2015. # Load the datasets created previously load(&quot;Data/master.Rda&quot;) master_wells &lt;- read.csv(file = &#39;Data/master_wells.csv&#39;) test_wells &lt;- read.csv(file = &#39;Data/test_wells.csv&#39;) test_ts_3mon &lt;- read.csv(file = &#39;Data/test_ts_3mon.csv&#39;) # Set date column as date test_ts_3mon[,&quot;date&quot;] &lt;- as.Date(test_ts_3mon[,&quot;date&quot;]) Next, we will loop through each well and each year and perform the IDW interpolation. We will use the idw function to do this. But first, we need to create a list of the dates on which we are going to interpolate. # Create the dates list that we want to interpolate dates &lt;- c(&quot;2012-01-01&quot;, &quot;2012-04-01&quot;, &quot;2012-07-01&quot;, &quot;2012-10-01&quot;, &quot;2013-01-01&quot;, &quot;2013-04-01&quot;, &quot;2013-07-01&quot;, &quot;2013-10-01&quot;, &quot;2014-01-01&quot;, &quot;2014-04-01&quot;, &quot;2014-07-01&quot;, &quot;2014-10-01&quot;, &quot;2015-01-01&quot;, &quot;2015-04-01&quot;, &quot;2015-07-01&quot;, &quot;2015-10-01&quot;) Using these dates, we will loop through each well at each date and perform the idw calculation, using the values from the adjacent wells. library(sp) library(gstat) library(dplyr) # Loop through and perform the IDW calculation for each well at each time step interp_values &lt;- as.data.frame(c()) for (well in test_wells$mergeOn){ for (dt in dates){ sample &lt;- test_ts_3mon %&gt;% filter(mergeOn != well) %&gt;% filter(date == dt) %&gt;% select(-lat, -lon, -Elev) %&gt;% merge(test_wells, by = &quot;mergeOn&quot;) %&gt;% select(mergeOn, date, lat, lon, Elev, Mean_depth, WTE) coordinates(sample) = ~lon+lat proj4string(sample) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) poi &lt;- data.frame( lon_poi = filter(test_wells, mergeOn == well)$lon, lat_poi = filter(test_wells, mergeOn == well)$lat) coordinates(poi) &lt;- ~ lon_poi + lat_poi proj4string(poi) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) new &lt;- idw(formula=WTE ~ 1, locations = sample, newdata = poi, idp = 2.0) interp_values &lt;- rbind(interp_values, c(well, dt, new@data$var1.pred)) } } Now we can clean up the dataframe a bit and create 2 new datasets that contain both interpolated vales and observed values. One will contain data from the well of interest, well 29N03W18M001M. The other will contain the observed and IDW values for all wells. colnames(interp_values) &lt;- c(&quot;mergeOn&quot;, &quot;date&quot;, &quot;WTE_IDW&quot;) interp_values$date &lt;- as.Date(interp_values$date) interp_values$WTE_IDW &lt;- as.numeric(interp_values$WTE_IDW) # Create 2 new datasets that combine observed values with IDW values comp_data_all_IDW &lt;- merge(interp_values,test_ts_3mon,by=c(&quot;mergeOn&quot;,&quot;date&quot;)) comp_data_single_IDW &lt;- test_ts_3mon %&gt;% filter(mergeOn == well) %&gt;% left_join(interp_values,by=c(&quot;date&quot;, &quot;mergeOn&quot;)) Finally, we will plot the IDW interpolated values. First, we will look at a single well, #29N03W18M001M. We will superimpose the IDW values over the entire observed time series for the well. We will do this using the GGPlot2 package again. # Rename the columns and set to proper data types # Plot all observed values vs. the IDW values for Well 29N03W18M001M library(ggplot2) comp_data_single_IDW %&gt;% ggplot(aes(x = date)) + geom_line(aes(y = WTE, color = &quot;steelblue&quot;)) + geom_line(aes(y = WTE_IDW, color = &quot;orange&quot;)) + scale_color_discrete(name = &quot;&quot;, labels = c(&quot;IDW Data&quot;, &quot;Observed Data&quot;)) + xlab(&quot;Date&quot;) + ylab(&quot;Water Table Elevation (ft above sea level)&quot;) + ggtitle(well) + theme(plot.title = element_text(hjust = 0.5), legend.position = &quot;bottom&quot;) Lastly, we will create a mosaic of time series data showing each well’s IDW values superimposed over the observed values for the interpolation period. # Plot 2012-2015 observed values vs. the IDW values comp_data_all_IDW %&gt;% ggplot(aes(x = date)) + geom_line(aes(y = WTE, color = &quot;steelblue&quot;)) + geom_line(aes(y = WTE_IDW, color = &quot;orange&quot;)) + scale_color_discrete(name = &quot;&quot;, labels = c(&quot;IDW Data&quot;, &quot;Observed Data&quot;)) + xlab(&quot;Date&quot;) + ylab(&quot;Water Table Elevation (ft above sea level)&quot;) + ggtitle(&quot;All Well Time Series: IDW vs. Observed&quot;) + theme(plot.title = element_text(hjust = 0.5), legend.position = &quot;bottom&quot;) + facet_wrap(~mergeOn) In the next section, we will explore the accuracy via error norms for these results. 5.3 Error Comparison The IDW method appears to be fairly accurate, with trends similar to the observed data. However, the magnitude of values are sometimes significantly different. It should again be noted that elevations (which were required to calculate water table elevations) were obtained manually using Google Earth. In addition, the lat/long coordinates for each well were not provided with sufficient resolution to have high confidence in any elevation measurement. Therefore, the water table elevations are subject to error. A table of error metrics can be calculated using the library Metrics. A root mean square error (RMSE) and an r-squared (R^2) error will be calculated for each well. The table will then be formatted for easy viewing. # Calculate a set of error metrics for each well&#39;s data library(Metrics) error_metrics_summary &lt;- data.frame() for (well in test_wells$mergeOn) { comp_data_well &lt;- comp_data_all_IDW %&gt;% filter(mergeOn == well) rmse_error &lt;-rmse(comp_data_well$WTE, comp_data_well$WTE_IDW) rsq &lt;- cor(comp_data_well$WTE, comp_data_well$WTE_IDW)^2 error_metrics_summary &lt;- rbind(error_metrics_summary, c(well, rmse_error, rsq)) } colnames(error_metrics_summary) &lt;- c(&quot;Well ID&quot;, &quot;RMSE&quot;, &quot;R^2&quot;) error_metrics_summary$RMSE &lt;- as.numeric(error_metrics_summary$RMSE) error_metrics_summary$&#39;R^2&#39; &lt;- as.numeric(error_metrics_summary$&#39;R^2&#39;) error_metrics_summary[,-1] &lt;-round(error_metrics_summary[,-1],3) error_metrics_summary ## Well ID RMSE R^2 ## 1 24N02W03B001M 19.151 0.102 ## 2 24N02W24D003M 22.423 0.236 ## 3 24N02W29N004M 36.415 0.165 ## 4 25N03W11B003M 6.639 0.338 ## 5 29N03W18M001M 7.510 0.192 Wells 25N03W11B003M and 29N03W18M001M appear to have values that are closest to their observed values. However, wells 24N02W24D003M and 25N03W11B003M appear to have the best “fit” to their observed values. This concludes the chapter on data imputation methods. The final chapter will summarize the paper and discuss future work. References "],
["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words This analysis has explored the data wrangling and imputation of groundwater data using a dataset from NASA’s JPL. We began by exploring the dataset and wrangling the data to find wells that we could perform data imputation on. We then explored two different methods for imputing data. The ELM method is very involved, and will require further analysis to be validated by this data. This paper has performed much of the data exploration and cleaning required and has laid the groundwork necessary for scaling this analysis to larger datasets. The IDW method was performed and returned reasonable results. The limitations inherent in this approach were evident in the error summary that we generated. Future analysis on this dataset would stem from further work on converting the ELM method to usable code in R. While outside the scope of this paper, validating this approach will be one of the objectives of the author as part of his Master’s thesis. Further error analysis will also be necessary, as this paper only uses two error metrics. Finally, the groundwork is in place for basic analysis of a few wells. Future analyses of this data could include scaling up this effort to include many more wells in the analysis and validation efforts of the ELM method. "],
["references.html", "References", " References "]
]
